<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="chapter-data-moves">
   <title>Data Moves: Transforming Data for Analysis</title>

   <introduction>
       <p>
           When working with real-world datasets, we rarely find data in exactly the form we need for analysis.
           Raw data often requires manipulation, transformation, and restructuring to answer our questions effectively.
           In this chapter, we explore <term>data moves</term> — the fundamental actions that alter a dataset's
           contents, structure, or values to support meaningful investigation.
       </p>
      
       <p>
           Data moves are essential skills that bridge the gap between having data and being able to use it effectively.
           They align with multiple strands of data science learning, including data creation and curation (Strand B),
           analysis techniques (Strand C), and preparation for visualization and communication (Strand E).
       </p>

       <p>
           <term>Learning Objectives:</term>
       </p>
       <p>
           By the end of this chapter, you will be able to:
           <ol>
               <li>Define data moves and explain their role in data analysis</li>
               <li>Identify and apply the six core data moves: filtering, grouping, summarizing, calculating, merging, and making hierarchy</li>
               <li>Transform datasets to support specific analytical goals</li>
               <li>Recognize when different data moves are appropriate for given situations</li>
               <li>Apply data moves using digital tools and understand their ethical implications</li>
           </ol>
       </p>
   </introduction>

   <section xml:id="section-what-are-data-moves">
       <title>What Are Data Moves?</title>

       <p>
           A <term>data move</term> is an action that alters a dataset's contents, structure, or values.
           Think of data moves as the essential toolkit for reshaping data to answer questions.
           Just as a carpenter uses different tools for different tasks, data analysts use different
           moves depending on their analytical goals.
       </p>

       <definition xml:id="def-data-move">
           <title>Data Move</title>
           <statement>
               <p>
                   An action that alters a dataset's:
                   <ul>
                       <li><term>Contents</term>: changing which cases (rows) or attributes (columns) are present</li>
                       <li><term>Structure</term>: changing how cases, attributes, and values relate to each other</li>
                       <li><term>Values</term>: changing the actual data values in cells</li>
                   </ul>
               </p>
           </statement>
       </definition>

       <example xml:id="example-card-metaphor">
           <title>The Deck of Cards Metaphor</title>
           <p>
               To understand data moves, imagine you have a deck of 800 cards, each representing a person
               from a health survey. Each card shows information like age, height, weight, and gender.
           </p>
          
           <p>
               If you want to explore height differences among 12-year-olds by gender, you might:
               <ol>
                   <li>Look through the deck to pick out all the 12-year-olds (filtering)</li>
                   <li>Separate the 12-year-olds into stacks by gender (grouping)</li>
                   <li>Calculate the average height for each stack (summarizing)</li>
               </ol>
           </p>

           <p>
               Each of these actions is a data move that transforms your data to help answer your question.
           </p>
       </example>

       <p>
           Data moves connect directly to several learning progression concepts:
           <ul>
               <li><em>Data Processing and Transformation</em> (B.1.3): Manipulating data through sorting, grouping, and filtering</li>
               <li><em>Tool Application</em> (C.4.1): Using digital tools to summarize and create visualizations</li>
               <li><em>Organizing and Structure</em> (B.1.2): Arranging data into structured formats using systematic methods</li>
           </ul>
       </p>
   </section>

   <section xml:id="section-core-data-moves">
       <title>The Six Core Data Moves</title>

       <introduction>
           <p>
               Research has identified six fundamental data moves that appear consistently across data analysis contexts.
               These moves can be combined and applied in various orders to transform data for specific analytical purposes.
           </p>
       </introduction>

       <subsection xml:id="subsection-filtering">
           <title>Filtering: Focusing Your Data</title>

           <p>
               <term>Filtering</term> creates a subset of data by removing cases that don't meet specific criteria.
               Filtering serves two main purposes: scoping (removing irrelevant data) and slicing (reducing complexity to gain insight).
           </p>

           <example xml:id="example-filtering-temperature">
               <title>Temperature Data Filtering</title>
               <p>
                   Consider a dataset with 17,500 temperature measurements taken every 30 minutes throughout 2000.
                   Plotting all data points shows a general relationship between air and soil temperature, but
                   filtering to show only one day (May 19, 2000) reveals a clear diurnal pattern that was
                   hidden in the full dataset.
               </p>
           </example>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Processing and Transformation (B.1.3): <q>Filter data based on groups or subsets relevant to the problem</q></li>
                   <li>Identifying Patterns (C.2.3): Using filtering to reveal relationships and trends</li>
                   <li>Sense-making with Visualizations (E.1.1): Creating visualizations to summarize complex ideas</li>
               </ul>
           </p>

           <activity xml:id="activity-filtering-practice">
               <title>Filtering Practice</title>
               <statement>
                   <p>
                       Using a dataset of student survey responses about technology use:
                       <ol>
                           <li>Filter to show only high school students</li>
                           <li>Filter to show only responses from the last month</li>
                           <li>Combine filters to show high school students from the last month who use social media daily</li>
                       </ol>
                       Discuss how each filter changes what patterns you can observe.
                   </p>
               </statement>
           </activity>
       </subsection>

       <subsection xml:id="subsection-grouping">
           <title>Grouping: Organizing for Comparison</title>

           <p>
               <term>Grouping</term> divides a dataset into multiple subsets based on shared characteristics.
               While filtering restricts analysis to a single subset, grouping creates multiple subsets for comparison.
               <term>Binning</term> is a special type of grouping that uses ranges of continuous values.
           </p>

           <p>
               Grouping is fundamental for making comparisons and is often a prerequisite for summarizing data.
               The way you define groups can dramatically affect what patterns emerge from your analysis.
           </p>

           <example xml:id="example-grouping-histogram">
               <title>Height Data Grouping</title>
               <p>
                   To create a histogram of height data, you group people into height ranges (bins).
                   Choosing narrow bins (5 cm ranges) versus wide bins (20 cm ranges) will reveal
                   very different patterns in the distribution.
               </p>
           </example>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Frequency Tables (C.1.4): <q>Organize data into frequency tables based on shared characteristics</q></li>
                   <li>Comparing Variables (C.2.1): <q>Use reasoning about distributions to compare two groups</q></li>
                   <li>Summarizing Groups (B.1.4): <q>Create summary measures for groups</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-summarizing">
           <title>Summarizing: Creating Aggregate Measures</title>

           <p>
               <term>Summarizing</term> produces aggregate values that describe groups or entire datasets.
               While the mean is common, summary measures include any statistic that consolidates multiple
               values into a single measure: median, mode, standard deviation, counts, percentages, etc.
           </p>

           <p>
               Summarizing often works together with grouping to create simpler displays that highlight overall patterns.
               However, this consolidation always involves information loss — when showing only measures of center,
               variability information disappears.
           </p>

           <example xml:id="example-summarizing-heights">
               <title>Height by Age and Gender</title>
               <p>
                   Starting with 800 individual height measurements, you can:
                   <ol>
                       <li>Group by age and gender (creating 30 groups)</li>
                       <li>Calculate mean height for each group (summarizing)</li>
                       <li>Create a graph with 30 points instead of 800, clearly showing growth patterns</li>
                   </ol>
               </p>
           </example>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Measures of Center (C.1.1): <q>Calculate summaries for categorical and numeric data</q></li>
                   <li>Measures of Spread (C.1.2): <q>Calculate standard deviation and interquartile range</q></li>
                   <li>Understanding Distributions (C.2.2): <q>Represent variability using numerical measures</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-calculating">
           <title>Calculating: Creating New Attributes</title>

           <p>
               <term>Calculating</term> creates new attributes (columns) using formulas applied to existing data.
               New attributes can be conceptual (like Body Mass Index combining height and weight) or
               convenience attributes (like converting units or creating categories).
           </p>

           <p>
               Calculated attributes expand analytical possibilities by:
               <ul>
                   <li>Combining multiple measurements into meaningful indices</li>
                   <li>Converting continuous variables into categories</li>
                   <li>Creating relative measures (percentages, ratios)</li>
                   <li>Standardizing units or formats</li>
               </ul>
           </p>

           <example xml:id="example-calculating-bmi">
               <title>Body Mass Index Calculation</title>
               <p>
                   From height (cm) and weight (kg) measurements, calculate BMI:
               </p>
               <p>
                   BMI = (weight / height²) × 10000
               </p>
               <p>
                   This new attribute provides health professionals with a standardized measure
                   that's more informative than weight alone.
               </p>
           </example>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Processing and Transformation (B.1.3): <q>Use calculations and logic statements to create new attributes</q></li>
                   <li>Defining Relationships (C.2.3): <q>Use data to create new groupings or construct new measures</q></li>
                   <li>Tool Application (C.4.1): <q>Perform data analysis using digital tools</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-merging">
           <title>Merging and Joining: Combining Datasets</title>

           <p>
               <term>Merging</term> and <term>joining</term> combine multiple datasets. Simple merging concatenates
               datasets about the same phenomenon from different sources. Joining adds new attributes about
               existing cases by connecting datasets through common identifiers.
           </p>

           <p>
               Real-world data often comes in separate tables that must be connected. Health surveys, for example,
               might store demographic information separately from medical measurements, requiring joins to
               create complete analytical datasets.
           </p>

           <example xml:id="example-joining-health-data">
               <title>Health Survey Data Joining</title>
               <p>
                   The NHANES health survey stores data in separate tables:
                   <ul>
                       <li><em>Demographics table</em>: ID, Age, Gender</li>
                       <li><em>Measurements table</em>: ID, Height, Weight, Blood Pressure</li>
                   </ul>
                  
                   Using the ID field, you can join these tables to analyze relationships
                   between demographic characteristics and health measurements.
               </p>
           </example>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Processing and Transformation (B.1.3): <q>Use an identifying attribute to merge datasets</q></li>
                   <li>Complexity of Structure (B.4.4): <q>Merge multiple datasets while maintaining appropriate case structure</q></li>
                   <li>Finding Secondary Data (B.2.4): <q>Combine multiple secondary datasets to create comprehensive data</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-making-hierarchy">
           <title>Making Hierarchy: Restructuring Data Relationships</title>

           <p>
               <term>Making hierarchy</term> organizes data into nested levels where some cases become <q>parents</q>
               containing other <q>child</q> cases. This move is particularly powerful because it provides an elegant
               way to think about grouping and summarizing.
           </p>

           <p>
               Hierarchical organization allows analysts to:
               <ul>
                   <li>Manipulate dataset structure for specific purposes</li>
                   <li>Create summary measures as first-class variables</li>
                   <li>Work with naturally nested data (students within schools, individuals within households)</li>
                   <li>Build more sophisticated analytical models</li>
               </ul>
           </p>

           <example xml:id="example-hierarchy-households">
               <title>Household Data Hierarchy</title>
               <p>
                   Census data includes both individual and household information. By creating a hierarchy
                   with households as parents and individuals as children, you can:
                   <ul>
                       <li>Calculate household size (count of individuals per household)</li>
                       <li>Analyze relationships between household characteristics and individual outcomes</li>
                       <li>Compare household-level patterns across different regions</li>
                   </ul>
               </p>
           </example>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Organizing and Structure (B.1.2): <q>Create hierarchical data structures with multiple related tables</q></li>
                   <li>Understanding Modeling (C.5.1): <q>Recognize that models simplify complex systems</q></li>
                   <li>Complexity of Structure (B.4.4): <q>Design data structures for longitudinal data and multiple aggregation levels</q></li>
               </ul>
           </p>
       </subsection>
   </section>

   <section xml:id="section-data-moves-in-practice">
       <title>Data Moves in Practice</title>

       <introduction>
           <p>
               Data moves rarely occur in isolation. Most analytical tasks require combining multiple moves
               in thoughtful sequences. The order and combination of moves depends on your data, your questions,
               and your analytical goals.
           </p>

           <example xml:id="example-complete-analysis">
               <title>Complete Analysis: Student Performance by School Type</title>
               <p>
                   Suppose you want to investigate whether student performance differs between public and private schools:
                  
                   <ol>
                       <li><em>Merging</em>: Combine student test scores with school information datasets</li>
                       <li><em>Filtering</em>: Remove students with missing test scores</li>
                       <li><em>Calculating</em>: Create standardized test score measures</li>
                       <li><em>Grouping</em>: Separate students by school type (public vs. private)</li>
                       <li><em>Summarizing</em>: Calculate mean scores and standard deviations for each group</li>
                       <li><em>Making Hierarchy</em>: Organize data with schools as parents and students as children to account for school-level effects</li>
                   </ol>
               </p>
           </example>
       </introduction>

       <subsection xml:id="subsection-choosing-moves">
           <title>Choosing Appropriate Data Moves</title>

           <p>
               Selecting the right data moves requires understanding both your data and your analytical goals.
               Consider these questions:
               <ul>
                   <li>What question am I trying to answer?</li>
                   <li>What is the current structure of my data?</li>
                   <li>What structure do I need for my intended analysis or visualization?</li>
                   <li>What information might I lose through each transformation?</li>
                   <li>How do my moves affect the interpretation of results?</li>
               </ul>
           </p>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Verifiable Questions (D.2.1): <q>Differentiate query-based, hypothesis-based, and causal questions</q></li>
                   <li>Problem Identification (D.2.1): <q>Construct data-based questions that explore relationships between variables</q></li>
                   <li>Iteration and Validation (D.2.2): <q>Regularly practice identifying alternative explanations for results</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-ethical-considerations">
           <title>Ethical Considerations in Data Moves</title>

           <p>
               Data moves are not neutral — they involve choices that can reinforce or challenge existing biases.
               When transforming data, consider:
               <ul>
                   <li>How do my grouping choices reflect or create social categories?</li>
                   <li>What voices or perspectives might be lost through filtering or summarizing?</li>
                   <li>How do calculated measures encode particular assumptions or values?</li>
                   <li>Who benefits from the insights enabled by these transformations?</li>
               </ul>
           </p>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Biases in Data (A.2.2): <q>Recognize how biases can obscure inferences drawn from data</q></li>
                   <li>Data Use Risks and Benefits (A.2.1): <q>Analyze how data use can perpetuate biases or systemic inequities</q></li>
                   <li>Ethics of Data Collection (B.3.3): <q>Design data collection methods that address privacy, consent, and fair representation</q></li>
               </ul>
           </p>
       </subsection>
   </section>

   <section xml:id="section-tools-for-data-moves">
       <title>Digital Tools for Data Moves</title>

       <introduction>
           <p>
               Modern data analysis relies on digital tools that automate and scale data moves. Different tools
               offer different capabilities and require different levels of technical expertise.
           </p>
       </introduction>

       <subsection xml:id="subsection-tool-categories">
           <title>Categories of Data Analysis Tools</title>

           <p>
               <ul>
                   <li><em>No-code tools</em>: Drag-and-drop interfaces (like CODAP, Tableau Public)</li>
                   <li><em>Low-code tools</em>: Visual programming with some formula writing (like Excel, Google Sheets)</li>
                   <li><em>High-code tools</em>: Programming languages (like R, Python, SQL)</li>
               </ul>
           </p>

           <p>
               The choice of tool depends on your data size, complexity, analytical needs, and technical skills.
               Learning data moves conceptually helps you transition between tools as your needs evolve.
           </p>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Tool Selection (C.4.4): <q>Select appropriate no-code, low-code, or high-code tools based on task</q></li>
                   <li>Tool Evaluation (C.4.3): <q>Assess technical limitations and compare different tool categories</q></li>
                   <li>The Role of Code (C.4.5): <q>Recognize advantages and limitations of coding versus no-code tools</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-reproducibility">
           <title>Reproducibility and Documentation</title>

           <p>
               Good data analysis requires documenting your data moves so others (including your future self)
               can understand and reproduce your work. This includes:
               <ul>
                   <li>Recording the sequence of moves applied</li>
                   <li>Explaining the reasoning behind each transformation</li>
                   <li>Preserving original data alongside transformed versions</li>
                   <li>Testing moves with subsets before applying to full datasets</li>
               </ul>
           </p>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Meta-analysis and Facts (D.3.7): <q>Document data analysis steps in shareable and reproducible format</q></li>
                   <li>Metadata (C.1.6): <q>Apply understanding of metadata to summarize and analyze data</q></li>
               </ul>
           </p>
       </subsection>
   </section>

   <section xml:id="section-beyond-core-moves">
       <title>Beyond the Core Moves</title>

       <introduction>
           <p>
               As data science continues to evolve, additional data moves become important for specific applications.
               These specialized moves build on the core six but address particular analytical needs.
           </p>
       </introduction>

       <subsection xml:id="subsection-data-cleaning-moves">
           <title>Data Cleaning Moves</title>

           <p>
               Real-world data requires extensive cleaning before analysis:
               <ul>
                   <li><em>Recoding</em>: Standardizing category names or value formats</li>
                   <li><em>Missing value handling</em>: Deciding how to treat incomplete data</li>
                   <li><em>Outlier management</em>: Identifying and addressing extreme values</li>
                   <li><em>Format conversion</em>: Changing data types or structures</li>
               </ul>
           </p>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Data Cleaning (B.1.1): <q>Apply comprehensive data validation procedures including automated checks</q></li>
                   <li>Cleanliness (B.4.1): <q>Work with datasets requiring multiple types of cleaning</q></li>
                   <li>Missing data (C.1.5): <q>Adjust analyses in light of missing values</q></li>
               </ul>
           </p>
       </subsection>

       <subsection xml:id="subsection-advanced-moves">
           <title>Advanced Analytical Moves</title>

           <p>
               Sophisticated analyses may require additional moves:
               <ul>
                   <li><em>Sampling</em>: Creating representative subsets for analysis or validation</li>
                   <li><em>Pivoting</em>: Converting between wide and long data formats</li>
                   <li><em>Time series operations</em>: Handling temporal data and trends</li>
                   <li><em>Text processing</em>: Extracting structured information from unstructured text</li>
               </ul>
           </p>

           <p>
               <em>Learning Progression Connections:</em>
               <ul>
                   <li>Analyzing Non-traditional Data (C.2.4): <q>Generate word clouds after standardizing and removing stop words</q></li>
                   <li>Machine Learning (C.2.5): <q>Use data to build decision trees and explore classification</q></li>
                   <li>Sampling and Simulation (D.1.5): <q>Use simulation-based inferential methods to draw conclusions</q></li>
               </ul>
           </p>
       </subsection>
   </section>

   <exercises xml:id="exercises-data-moves">
       <title>Chapter Exercises</title>

       <exercise xml:id="exercise-identify-moves">
           <title>Identifying Data Moves</title>
           <statement>
               <p>
                   For each scenario below, identify which data move(s) would be most appropriate and explain your reasoning:
                  
                   <ol>
                       <li>You have temperature data from 50 weather stations, but you only want to analyze data from coastal locations.</li>
                       <li>You want to compare average rainfall between northern and southern regions of your state.</li>
                       <li>You have separate datasets for student demographics and test scores, both containing student ID numbers.</li>
                       <li>You want to create age categories (child, teen, adult, senior) from a dataset with exact birth dates.</li>
                       <li>You want to analyze how household income relates to individual education levels using census data.</li>
                   </ol>
               </p>
           </statement>
       </exercise>

       <exercise xml:id="exercise-sequence-moves">
           <title>Sequencing Data Moves</title>
           <statement>
               <p>
                   Design a sequence of data moves to answer this question: <q>Do students at larger schools perform better on standardized tests than students at smaller schools?</q>
                  
                   Available data:
                   <ul>
                       <li>Student test scores with student IDs</li>
                       <li>School enrollment data with school IDs</li>
                       <li>Student-school assignments linking student IDs to school IDs</li>
                   </ul>
                  
                   For each move in your sequence, explain:
                   <ol>
                       <li>Which specific data move you're applying</li>
                       <li>Why this move is necessary</li>
                       <li>How it prepares the data for subsequent steps</li>
                   </ol>
               </p>
           </statement>
       </exercise>

       <exercise xml:id="exercise-ethical-analysis">
           <title>Ethical Analysis of Data Moves</title>
           <statement>
               <p>
                   Consider a dataset about job applications that includes information about applicant names,
                   education, experience, and hiring decisions. You want to analyze whether there are
                   differences in hiring rates across different groups.
                  
                   <ol>
                       <li>What data moves would you need to perform this analysis?</li>
                       <li>What ethical considerations arise from each move?</li>
                       <li>How might your choices in performing these moves affect your conclusions?</li>
                       <li>What steps could you take to ensure your analysis is fair and transparent?</li>
                   </ol>
               </p>
           </statement>
       </exercise>

       <exercise xml:id="project-complete-analysis">
           <title>Complete Data Analysis Project</title>
           <statement>
               <p>
                   Choose a publicly available dataset relevant to your classroom and what your students' interests are (suggestions: sports, social media, education, health,
                   environment, economics). Conduct a complete analysis that demonstrates all six core data moves and think about how you can not only teach these steps to your students (depending on age), but also how you can now use this dataset in your classroom going forward to achieve some of your learning objectives.
               </p>
              
               <p>
                   Your analysis should include:
                   <ol>
                       <li>Plan: Formulate a clear research question</li>
                       <li>Document: Keep a log of every data move you perform</li>
                       <li>Implement: Apply filtering, grouping, summarizing, calculating, merging (if applicable), and hierarchy creation</li>
                       <li>Reflect: Analyze how each move contributed to answering your question</li>
                       <li>Communicate: Present your findings with clear visualizations and explanations</li>
                   </ol>
               </p>
           </statement>
       </exercise>
   </exercises>

   <conclusion>
       <title>Chapter Summary</title>

       <p>
           Data moves are the fundamental building blocks of data analysis, providing the tools to transform
           raw data into forms that support meaningful investigation. The six core moves — filtering, grouping,
           summarizing, calculating, merging, and making hierarchy — can be combined in countless ways to
           address diverse analytical questions.
       </p>

       <p>
           Mastering data moves requires both technical skills and conceptual understanding. You need to know
           not just how to perform these transformations, but when to use them, how they affect your data,
           and what ethical considerations they raise. This understanding becomes increasingly important as
           data plays a larger role in society and decision-making.
       </p>

       <p>
           The learning progressions provide a roadmap for developing these skills across educational levels,
           from basic sorting and counting in elementary grades to sophisticated analytical techniques in
           advanced courses. By grounding data moves in this developmental framework, we can build students'
           capacity to engage critically and creatively with data throughout their lives.
       </p>

       <p>
           As you continue your data science journey, remember that data moves are means to an end — they help
           you explore questions, test hypotheses, and communicate insights. The most important skill is
           learning to ask good questions and then selecting and sequencing data moves to help answer them
           in ways that are accurate, ethical, and meaningful.
       </p>
   </conclusion>
</chapter>
